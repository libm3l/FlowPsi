//#############################################################################
//#
//# Copyright 2015-2019, Mississippi State University
//#
//# This file is part of the flowPsi computational fluid dynamics solver.
//#
//# The flowPsi solver is free software: you can redistribute it and/or modify
//# it under the terms of the GNU General Public License as published by
//# the Free Software Foundation, either version 3 of the License, or
//# (at your option) any later version.
//#
//# The flowPsi solver is distributed in the hope that it will be useful,
//# but WITHOUT ANY WARRANTY; without even the implied warranty of
//# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
//# GNU General Public License for more details.
//#
//# You should have received a copy of the GNU General Public License
//# along with the flowPsi solver.  If not, see <http://www.gnu.org/licenses>
//#
//#############################################################################

// This file implements the particle_base module (obviously)
#include "particle_base.h"

using std::cout ;
using std::cerr ;
using std::endl ;
using std::vector ;
using std::list ;
using std::map ;
using std::set ;
using std::string ;
#include <utility>
using std::pair ;
#include <algorithm>

////////////////////////////////////////////////////
namespace lagrangianP {

  ///////////////////////////////////////////////////////////////////
  // this section implements ParticleSpace::Geometry
  ///////////////////////////////////////////////////////////////////
  void ParticleSpace::Geometry::
  init_local_geom
  (const const_store<vector<MeshInteriorFaceTopo> >& mift,
   const const_store<vector<MeshBoundaryFaceTopo> >& mbft
#ifdef CHECK_PARTICLE_WALK
   ,const const_store<double>& cs,
   const const_store<Loci::Array<double,6> >& cb
#endif
   ) {
    // cache local Geometry is easy, setup the rep for the cellcenter
    local_cellcenter.setRep
      (factsP->get_variable("cellcenter")) ;
    // and then copy the generated interior and boundary topology.
    // NOTE: we don't choose to setup the reps for them because
    // the interior and boundary topology were generated by a runtime
    // Loci rule (in other words, they are not installed in the
    // fact_db as initial facts), and therefore are subject to
    // be deleted by the runtime system.  We need to copy them
    // here in order to make sure that the caches here are valid.
    entitySet dom = mift.domain() ;
    local_cell_int_face_topo.allocate(dom) ;
    for(entitySet::const_iterator b=dom.begin(),e=dom.end();
        b!=e;++b)
      local_cell_int_face_topo[*b] = mift[*b] ;

    dom = mbft.domain() ;
    local_cell_bnd_face_topo.allocate(dom) ;
    for(entitySet::const_iterator b=dom.begin(),e=dom.end();
        b!=e;++b)
      local_cell_bnd_face_topo[*b] = mbft[*b] ;

#ifdef CHECK_PARTICLE_WALK
    dom = cs.domain() ;
    cell_sphere.allocate(dom) ;
    for(entitySet::const_iterator b=dom.begin(),e=dom.end();
        b!=e;++b)
      cell_sphere[*b] = cs[*b] ;

    dom = cb.domain() ;
    cell_box.allocate(dom) ;
    for(entitySet::const_iterator b=dom.begin(),e=dom.end();
        b!=e;++b)
      cell_box[*b] = cb[*b] ;
#endif
    // in case of a sequential run, we will also setup the
    // dynamic cache to be the same as the local records
    // so that later in the code, we don't have to distinguish
    // the difference between the two types
    if(!parallel_run) {
      dynamic_cell_int_face_topo.setRep
        (local_cell_int_face_topo.Rep()) ;
      dynamic_cell_bnd_face_topo.setRep
        (local_cell_bnd_face_topo.Rep()) ;
      // also build two identity maps
      entitySet all_cells = local_cellcenter.domain() ;
      l2g.allocate(all_cells) ;
      g2l.allocate(all_cells) ;
      for(entitySet::const_iterator ei=all_cells.begin();
          ei!=all_cells.end();++ei) {
        l2g[*ei] = *ei ;
        g2l[*ei] = *ei ;
      }
    }
  }

  void ParticleSpace::Geometry::recache_local() {
    // obtaining the local geometric data first from the fact_db
    store<Loci::Area> area(factsP->get_variable("area{n}")) ;
    store<vec3d> facecenter(factsP->get_variable("facecenter{n}")) ;
    store<vec3d> cellcenter(factsP->get_variable("cellcenter{n}")) ;
    store<byte_t> iblank(factsP->get_variable("iblank{n}")) ;
    Map cl(factsP->get_variable("cl")) ;
    Map cr(factsP->get_variable("cr")) ;
    multiMap upper(factsP->get_variable("upper")) ;
    multiMap lower(factsP->get_variable("lower")) ;
    multiMap boundary_map(factsP->get_variable("boundary_map")) ;
    store<int>
      particle_wall_bc(factsP->get_variable("particle_wall_bc")) ;
#ifdef CHECK_PARTICLE_WALK
    multiMap face2node(factsP->get_variable("face2node")) ;
    store<vec3d> pos(factsP->get_variable("pos")) ;
#endif
    entitySet dom = cellcenter.domain() ;
    if(parallel_run) {
      Loci::fact_db::distribute_infoP df = factsP->get_distribute_info() ;
      dom &= df->my_entities ;
    }
    // reallocate local cache
    local_cell_int_face_topo.allocate(dom) ;
    local_cell_bnd_face_topo.allocate(dom) ;      

    if(parallel_run) {
      Loci::fact_db::distribute_infoP df = factsP->get_distribute_info() ;
      for(entitySet::const_iterator ei=dom.begin();ei!=dom.end();++ei) {
        int local_cell = *ei ;
        vector<MeshInteriorFaceTopo>& vmift =
          local_cell_int_face_topo[local_cell] ;
        vector<MeshBoundaryFaceTopo>& vmbft =
          local_cell_bnd_face_topo[local_cell] ;
        vmift.clear() ;
        vmbft.clear() ;
        // loop over lower
        int lower_size = lower[local_cell].size() ;
        for(int i=0;i<lower_size;++i) {
          int local_face = lower[local_cell][i] ;
          // see if we need to flip the face_normal
          vec3d face_normal = area[local_face].n ;
          vec3d aux = cellcenter[local_cell] - facecenter[local_face] ;
          double flip = (dot(face_normal,aux)>=0) ? 1.0 : (-1.0) ;
          face_normal = flip * face_normal ;

          vmift.push_back
            (MeshInteriorFaceTopo(df->l2g[local_face],
                                  facecenter[local_face],
                                  face_normal,
                                  df->l2g[cl[local_face]],
                                  iblank[cl[local_face]])) ;
        }
        // loop over upper
        int upper_size = upper[local_cell].size() ;
        for(int i=0;i<upper_size;++i) {
          int local_face = upper[local_cell][i] ;

          vec3d face_normal = area[local_face].n ;
          vec3d aux = cellcenter[local_cell] - facecenter[local_face] ;
          double flip = (dot(face_normal,aux)>=0) ? 1.0 : (-1.0) ;
          face_normal = flip * face_normal ;

          vmift.push_back
            (MeshInteriorFaceTopo(df->l2g[local_face],
                                  facecenter[local_face],
                                  face_normal,
                                  df->l2g[cr[local_face]],
                                  iblank[cr[local_face]])) ;
        }
        // loop over boundary_map
        int bnd_map_size = boundary_map[local_cell].size() ;
        for(int i=0;i<bnd_map_size;++i) {
          int bnd_face = boundary_map[local_cell][i] ;
          int wall_type = particle_wall_bc[bnd_face] ;

          vec3d face_normal = area[bnd_face].n ;
          vec3d aux = cellcenter[local_cell] - facecenter[bnd_face] ;
          double flip = (dot(face_normal,aux)>=0) ? 1.0 : (-1.0) ;
          face_normal = flip * face_normal ;

          vmbft.push_back(MeshBoundaryFaceTopo(df->l2g[bnd_face],
                                               facecenter[bnd_face],
                                               face_normal,
                                               wall_type)) ;
        }
      }
    } else {
      // sequential case
      for(entitySet::const_iterator ei=dom.begin();ei!=dom.end();++ei) {
        int local_cell = *ei ;
        vector<MeshInteriorFaceTopo>& vmift =
          local_cell_int_face_topo[local_cell] ;
        vector<MeshBoundaryFaceTopo>& vmbft =
          local_cell_bnd_face_topo[local_cell] ;
        vmift.clear() ;
        vmbft.clear() ;
        // loop over lower
        int lower_size = lower[local_cell].size() ;
        
        for(int i=0;i<lower_size;++i) {
          int local_face = lower[local_cell][i] ;

          vec3d face_normal = area[local_face].n ;
          vec3d aux = cellcenter[local_cell] - facecenter[local_face] ;
          double flip = (dot(face_normal,aux)>=0) ? 1.0 : (-1.0) ;
          face_normal = flip * face_normal ;

          vmift.push_back(MeshInteriorFaceTopo(local_face,
                                               facecenter[local_face],
                                               face_normal,
                                               cl[local_face],
                                               iblank[cl[local_face]])) ;
        }
        // loop over upper
        int upper_size = upper[local_cell].size() ;
        for(int i=0;i<upper_size;++i) {
          int local_face = upper[local_cell][i] ;

          vec3d face_normal = area[local_face].n ;
          vec3d aux = cellcenter[local_cell] - facecenter[local_face] ;
          double flip = (dot(face_normal,aux)>=0) ? 1.0 : (-1.0) ;
          face_normal = flip * face_normal ;

          vmift.push_back(MeshInteriorFaceTopo(local_face,
                                               facecenter[local_face],
                                               face_normal,
                                               cr[local_face],
                                               iblank[cr[local_face]])) ;
        }
        // loop over boundary_map
        int bnd_map_size = boundary_map[local_cell].size() ;
        for(int i=0;i<bnd_map_size;++i) {
          int bnd_face = boundary_map[local_cell][i] ;
          int wall_type = particle_wall_bc[bnd_face] ;

          vec3d face_normal = area[bnd_face].n ;
          vec3d aux = cellcenter[local_cell] - facecenter[bnd_face] ;
          double flip = (dot(face_normal,aux)>=0) ? 1.0 : (-1.0) ;
          face_normal = flip * face_normal ;

          vmbft.push_back(MeshBoundaryFaceTopo(bnd_face,
                                               facecenter[bnd_face],
                                               face_normal,
                                               wall_type)) ;
        }
      }
    }
#ifdef CHECK_PARTICLE_WALK
    local_cell_sphere.allocate(dom) ;
    local_cell_box.allocate(dom) ;
    for(entitySet::const_iterator ei=dom.begin();ei!=dom.end();++ei) {
      Entity cell = *ei ;
      // collect defining nodes for each cell
      vector<Entity> cell2node ;
      // lower faces first
      int mm_size = lower[cell].size() ;
      for(int i=0;i<mm_size;++i) {
        Entity f = lower[cell][i] ;
        int s = face2node[f].size() ;
        for(int k=0;k<s;++k)
          cell2node.push_back(face2node[f][k]) ;
      }
      // upper faces
      mm_size = upper[cell].size() ;
      for(int i=0;i<mm_size;++i) {
        Entity f = upper[cell][i] ;
        int s = face2node[f].size() ;
        for(int k=0;k<s;++k)
          cell2node.push_back(face2node[f][k]) ;
      }
      // boundary faces
      mm_size = boundary_map[cell].size() ;
      for(int i=0;i<mm_size;++i) {
        Entity f = boundary_map[cell][i] ;
        int s = face2node[f].size() ;
        for(int k=0;k<s;++k)
          cell2node.push_back(face2node[f][k]) ;
      }
      // remove duplicate nodes
      std::sort(cell2node.begin(),cell2node.end()) ;
      vector<Entity>::iterator new_end =
        std::unique(cell2node.begin(),cell2node.end()) ;
      // compute the radius of the sphere and the bounding box
      const vec3d& cc = cellcenter[cell] ;
      double r = std::numeric_limits<double>::min() ;
      double x_min = std::numeric_limits<double>::max() ;
      double x_max = std::numeric_limits<double>::min() ;
      double y_min = std::numeric_limits<double>::max() ;
      double y_max = std::numeric_limits<double>::min() ;
      double z_min = std::numeric_limits<double>::max() ;
      double z_max = std::numeric_limits<double>::min() ;

      for(vector<Entity>::iterator vi=cell2node.begin();
          vi!=new_end;++vi) {
        double local_r = norm(pos[*vi] - cc) ;
        if(local_r > r)
          r = local_r ;
        double x = pos[*vi].x ;
        double y = pos[*vi].y ;
        double z = pos[*vi].z ;
        if(x < x_min)
          x_min = x ;
        if(x > x_max)
          x_max = x ;
        if(y < y_min)
          y_min = y ;
        if(y > y_max)
          y_max = y ;
        if(z < z_min)
          z_min = z ;
        if(z > z_max)
          z_max = z ;
      }
      local_cell_sphere[cell] = 2*r ;
      local_cell_box[cell][0] = x_min ;
      local_cell_box[cell][1] = x_max ;
      local_cell_box[cell][2] = y_min ;
      local_cell_box[cell][3] = y_max ;
      local_cell_box[cell][4] = z_min ;
      local_cell_box[cell][5] = z_max ;
    }
    local_cellcenter.allocate(dom) ;
    for(entitySet::const_iterator b=dom.begin(),e=dom.end();
        b!=e;++b)
      local_cellcenter[*b] = cellcenter[*b] ;
#endif
    if(!parallel_run) {
      dynamic_cell_int_face_topo.setRep
        (local_cell_int_face_topo.Rep()) ;
      dynamic_cell_bnd_face_topo.setRep
        (local_cell_bnd_face_topo.Rep()) ;
#ifdef CHECK_PARTICLE_WALK
      dynamic_cell_sphere.setRep(local_cell_sphere.Rep()) ;
      dynamic_cell_box.setRep(local_cell_box.Rep()) ;
      dynamic_cellcenter.setRep(local_cellcenter.Rep()) ;
#endif
      // also build two identity maps
      entitySet all_cells = local_cell_int_face_topo.domain() ;
      l2g.allocate(all_cells) ;
      g2l.allocate(all_cells) ;
      for(entitySet::const_iterator ei=all_cells.begin();
          ei!=all_cells.end();++ei) {
        l2g[*ei] = *ei ;
        g2l[*ei] = *ei ;
      }
    }
    // finally empty the dynamic cache
    if(parallel_run) {
      resize(dom.size()) ;
      cells_cached = EMPTY ;
    }
  }

  void ParticleSpace::Geometry::
  resize(size_t sz) {
    cache_size = sz ;
    cells_cached = EMPTY ;
    if(sz == 0)
      cache_domain = EMPTY ;
    else
      cache_domain = entitySet(interval(0,cache_size-1)) ;

    //dynamic_cellcenter.allocate(cache_domain) ;
    dynamic_cell_int_face_topo.allocate(cache_domain) ;
    dynamic_cell_bnd_face_topo.allocate(cache_domain) ;
    l2g.allocate(cache_domain) ; g2l.allocate(EMPTY) ;
  }

  void ParticleSpace::Geometry::
  compact(const entitySet& valid) {
    // first figure out the "real" valid cell cache
    cells_cached = cells_cached & valid ;
    // then compact the buffer
    // move memory to the beginning and remap them
    vector<bool> valid_record(cache_size,false) ;
    for(entitySet::const_iterator ei=cells_cached.begin();
        ei!=cells_cached.end();++ei)
      valid_record[g2l[*ei]] = true ;

    size_t dst_idx = 0 ;
    for(size_t i=0;i<valid_record.size();++i) {
      // invalid cache records needn't be moved
      if(!valid_record[i])
        continue ;

      size_t src_idx = i ;

      // if destination index equals the source index
      // then we don't need to move the data
      if(dst_idx == src_idx) {
        ++dst_idx ;
        continue ;
      }

      //dynamic_cellcenter[dst_idx] =
      //dynamic_cellcenter[src_idx] ;
      
      dynamic_cell_int_face_topo[dst_idx] =
        dynamic_cell_int_face_topo[src_idx] ;
      
      dynamic_cell_bnd_face_topo[dst_idx] =
        dynamic_cell_bnd_face_topo[src_idx] ;
      // remap
      Entity global = l2g[src_idx] ;
      l2g[dst_idx] = global ;
      g2l[global] = dst_idx ;
      
      ++dst_idx ;
    }
  }

  void ParticleSpace::Geometry::
  simple_cache(const entitySet& request, int start) {
    // first we need to generate the new remaps for the request part
    int remap_idx = start ;
    for(entitySet::const_iterator ei=request.begin();
        ei!=request.end();++ei) {
      g2l[*ei] = remap_idx ;
      l2g[remap_idx] = *ei ;
      ++remap_idx ;
    }
    cells_cached += request ;

    // cache the contents actually
    Loci::fact_db::distribute_infoP df = factsP->get_distribute_info() ;
    typedef ParticleSpace::Communicator::P2pCommInfo P2pCommInfo ;
    vector<P2pCommInfo> send, recv ;
    const vector<entitySet>& ptn = factsP->get_init_ptn() ;    
    vector<entitySet> dist(ptn.size()), dist_t(ptn.size()) ;
    for(size_t i=0;i<ptn.size();++i)
      dist[i] = request & ptn[i] ;
    transpose_vector_entitySet_opt(dist, dist_t) ;
    for(size_t i=0;i<ptn.size();++i) {
      const entitySet& es = dist[i] ;
      if(es != EMPTY)
        recv.push_back(P2pCommInfo(i,es,remap_entitySet(es,g2l))) ;
      const entitySet& est = dist_t[i] ;
      if(est != EMPTY)
        send.push_back(P2pCommInfo(i,est,remap_entitySet(est,df->g2l))) ;
    }

    vector<Loci::storeRepP> in, out ;
    //in.push_back(local_cellcenter.Rep()) ;
    in.push_back(local_cell_int_face_topo.Rep()) ;
    in.push_back(local_cell_bnd_face_topo.Rep()) ;

    out.push_back(dynamic_cell_int_face_topo.Rep()) ;
    out.push_back(dynamic_cell_bnd_face_topo.Rep()) ;

    get_remote_stores_inc(in, send, recv, df->l2g, g2l, out) ;
  }

  void ParticleSpace::Geometry::
  cache(const entitySet& request) {
    if(!parallel_run)
      return ;                  // do nothing in the sequential run
    // first find out what to cache.
    entitySet missing = request - cells_cached ;
    // only cache if missing is non-empty (globally)
    if(Loci::GLOBAL_AND(missing==EMPTY))
      return ;
    // otherwise, we have something to cache.
    // first determine if there is enough space in the buffer
    // to receive the additional data for the missing cells
    size_t request_size = missing.size() ;
    if(request_size > available_slots()) {
      // if buffer compaction is feasible, do it,
      // otherwise resize the entire cache
      entitySet valid = request & cells_cached ;
      size_t valid_size = valid.size() ;
      if(cache_size - valid_size >= request_size) {
        // do a compaction
        compact(valid) ;
      } else {
        // do a cache resize, resize nees to be performed on
        // the original request set, and we also set the cache
        // size to be twice larger than that
        missing = request ;
        size_t new_cache_size = 2 * missing.size() ;
        resize(new_cache_size) ;
      }
    }
    // at this point, we are sure that there is enough
    // space in the buffer to hold the missing part
    simple_cache(missing, cells_cached.size()) ;
  }

  ////////////////////////////////////////////////////////////////
  // this section implements ParticleSpace::MappingInfo methods
  ////////////////////////////////////////////////////////////////
  void ParticleSpace::MappingInfo::
  update_mapping_info(const entitySet& new_cells) {
    if(!parallel_run) {
      // in a sequential environment, the mapping info
      // is not important since it is not really useful
      // hence, we just update the domain and return
      cells = new_cells ;
      cells_l = new_cells ;
      return ;
    }
    // first we'll need to compute the cells that are new,
    // and the cells that will be removed from the current set
    entitySet add = new_cells - cells ;
    entitySet remove = cells - new_cells ;
    if(add.size() >= remove.size()) {
      // this case handles when the number of added new cells
      // is larger than or equal to the cells to be removed.
      // then we need to expanded the mapping info.

      // first we need to reuse the the removed local numbering
      vector<Entity> added_v ;
      entitySet::const_iterator eia, eir ;
      eia = add.begin() ;
      for(eir=remove.begin();eir!=remove.end();++eir,++eia) {
        Entity reuse = g2l[*eir] ;
        g2l[*eia] = reuse ;
        l2g[reuse] = *eia ;
        added_v.push_back(*eia) ;
      }
      // erase the remove set
      if(remove != EMPTY)
        g2l.erase(remove) ;
      // we will then see if the local numbering buffer is
      // large enough to add the remaining new cells
      size_t remaining_add_size = add.size() - remove.size() ;
      if(remaining_add_size >
         static_cast<size_t>(local_unused.size())) {
        // not enough space, we need to reallocate the local
        // numbering mapping info completely.
        // just like std::vector, we will double the allocation
        // size everytime.
        size_t old_size = l2g.domain().size() ;
        size_t new_size = 2 * (remaining_add_size + old_size) ;
        // NOTE: it is important that we NOT compacting the
        // l2g map because we want to keep all the "old" records
        // the same as before.

        // backup the old records
        vector<Loci::int_type> l2g_backup(old_size) ;
        for(size_t k=0;k<old_size;++k)
          l2g_backup[k] = l2g[k] ;
        
        l2g.allocate(entitySet(interval(0,new_size-1))) ;

        for(size_t k=0;k<old_size;++k)
          l2g[k] = l2g_backup[k] ;

        local_unused += entitySet(interval(old_size,new_size-1)) ;
      }
      // finally we need to add those remaining new cells
      vector<Entity> reused_v ;
      entitySet::const_iterator eiu = local_unused.begin() ;
      for(;eia!=add.end();++eia,++eiu) {
        g2l[*eia] = *eiu ;
        l2g[*eiu] = *eia ;
        reused_v.push_back(*eiu) ;
      }
      entitySet reused = Loci::create_intervalSet(reused_v.begin(),
                                                  reused_v.end()) ;
      cells_l += reused ;
      local_unused -= reused ;
      // end this branch...
    } else {
      // this last case handles when the number of added new cells
      // is less than the number of cells to be removed, then we
      // will need to shrink the mapping info.

      // first step is same as the above branch -- reuse the removed
      entitySet::const_iterator eia, eir ;
      eia = add.begin() ; eir = remove.begin() ;
      for(;eia!=add.end();++eia,++eir) {
        Entity reuse = g2l[*eir] ;
        g2l[*eia] = reuse ;
        l2g[reuse] = *eia ;
      }
      // recycle the remaining remove cells to local_unused
      vector<Entity> remaining_remove_v ;
      for(;eir!=remove.end();++eir)
        remaining_remove_v.push_back(g2l[*eir]) ;
      // erase the remove set
      if(remove != EMPTY)
        g2l.erase(remove) ;

      entitySet new_unused =
        Loci::create_intervalSet(remaining_remove_v.begin(),
                                 remaining_remove_v.end()) ;
      cells_l -= new_unused ;
      local_unused += new_unused ;      
    } // end if/else

    cells = new_cells ;

    // NOTE: actually we cannot compact the local numbering
    // mapping since it is necessary to keep all the "old"
    // records the same, because other places have already
    // assumed such relations, e.g., the communicator structure.
  }

  void ParticleSpace::MappingInfo::
  cache_loci_maps(Loci::fact_db* factsP) {
    if(factsP->is_distributed_start()) {
      // cache df->g2l, and df->l2g
      Loci::fact_db::distribute_infoP df =
        factsP->get_distribute_info() ;
      loci_g2l.setRep(df->g2l.Rep()) ;
      loci_l2g.setRep(df->l2g.Rep()) ;
      loci_g2l_dom = loci_g2l.domain() ;
      loci_l2g_dom = loci_l2g.domain() ;
    } else {
      // build two identity maps
      loci_g2l_dom = (factsP->get_init_ptn())[0] ;
      loci_l2g_dom = loci_g2l_dom ;
      loci_g2l.allocate(loci_g2l_dom) ;
      loci_l2g.allocate(loci_l2g_dom) ;
      for(entitySet::const_iterator ei=loci_g2l_dom.begin();
          ei!=loci_g2l_dom.end();++ei) {
        loci_g2l[*ei] = *ei ;
        loci_l2g[*ei] = *ei ;
      }
      // g2l and l2g also shares the identity maps
      g2l.setRep(loci_g2l.Rep()) ;
      l2g.setRep(loci_l2g.Rep()) ;
    }
  }
  
  ////////////////////////////////////////////////////////////////
  // this section implements ParticleSpace::Communicator methods
  ////////////////////////////////////////////////////////////////

  namespace {
    // utility function to merge two vector<P2pCommInfo>
    // y is added to x.
    typedef ParticleSpace::Communicator::P2pCommInfo P2pCommInfoUN ;
    void merge_p2p_vector(vector<P2pCommInfoUN>& x,
                          vector<P2pCommInfoUN> y) {
      // to merge the vectors, we do so by sorting the new
      // vectors and based on the process id. then we merge
      // records according to the process id in the vectors

      std::sort(x.begin(), x.end()) ;
      std::sort(y.begin(), y.end()) ;

      size_t x_len = x.size() ;
      size_t x_cur = 0 ;
      size_t y_len = y.size() ;
      size_t y_cur = 0 ;

      while(true) {
        // termination conditions
        if(y_cur >= y_len) {
          // running out of y, just break
          break ;
        }
        if(x_cur >= x_len) {
          // running out of x, append the
          // remaining y to the x vector
          for(;y_cur!=y_len;++y_cur)
            x.push_back(y[y_cur]) ;
          break ;
        }
        // 3 situations
        if(x[x_cur].proc < y[y_cur].proc)
          ++x_cur ;
        else if(x[x_cur].proc == y[y_cur].proc) {
          // merge
          x[x_cur] += y[y_cur] ;
          ++x_cur ; ++y_cur ;
        } else {
          // x[x_cur].proc > y[y_cur].proc
          // then append the current y record to x
          x.push_back(y[y_cur]) ;
          ++y_cur ;
        }
      }
      std::sort(x.begin(), x.end()) ;
    }
  } // end of namespace "unnamed"
  
  ParticleSpace::Communicator& ParticleSpace::Communicator::
  operator+=(const Communicator& c) {

    merge_p2p_vector(send, c.send) ;
    merge_p2p_vector(recv, c.recv) ;

    send_alloc += c.send_alloc ;
    recv_alloc += c.recv_alloc ;

    // technically we should also merge the pack/unpack
    // maps, but since the current implementation only
    // caches the pack/unpack maps, and it is always
    // that the maps are being updated first, so we
    // don't actually merge the maps here. however,
    // doing so relies on the assumption that the
    // ParticleSpace.cell_mapping is always updated
    // before this "+=" operator is called. thus this
    // method may be dangerous for general use, therefore
    // we mark it private and only allows the
    // ParticleSpace to use it

    // because the current pack/unpack map might not
    // have been initialized yet, we will inherit the
    // maps from the passed in communicator

    pack.setRep(c.pack.Rep()) ;
    unpack.setRep(c.unpack.Rep()) ;

    return *this ;
  }

  ////////////////////////////////////////////////////////////////
  // this section implements ParticleSpace methods
  ////////////////////////////////////////////////////////////////
  ParticleSpace::ParticleSpace()

    :factsP(Loci::exec_current_fact_db),
     face_history(0), face_history_tracing_start(9),
     max_walking_step_warning(500), dot_threshold(0),
     particle_redistribution_mean_max_threshold(1.1),
     particle_redistribution_freq(250), particle_number(0),
     particle_id_alloc(0), last_location_steps(0) {

    parallel_run = factsP->is_distributed_start() ;
    cell_mapping.parallel_run = parallel_run ;
    cell_mapping.cache_loci_maps(factsP) ;
  }

  // methods to update the communicator object.

  void ParticleSpace::
  update_comm(const entitySet& domain,
              Communicator& pull, Communicator& push) {
    typedef Communicator::P2pCommInfo P2pCommInfo ;
    // obtaining the Loci::fact_db's entity partition
    const std::vector<Loci::entitySet>& ptn =
      factsP->get_init_ptn() ;
    // clear the previous comm
    pull.clear() ;
    push.clear() ;
    // obtain entity set partition of the passed in "domain"
    std::vector<Loci::entitySet> dist(ptn.size()) ;
    for(size_t i=0;i<ptn.size();++i) {
      dist[i] = domain & ptn[i] ;
    }
    // then obtain the transpose of the dist
    std::vector<Loci::entitySet> dist_t(ptn.size()) ;
    transpose_vector_entitySet_opt(dist, dist_t) ;

    // to the pull_comm,
    // dist is the receiving set and dist_t is the sending set

    // to the push_comm,
    // dist is the sending set and dist_t is the receiving set.

    // to the pull_comm,
    // the pack map is loci_l2g, this is because that the pull_comm
    // is responsible to send some of its local entities to
    // others. the unpack map is cell_mapping.g2l, this is because
    // the receiving entities are the dynamic particle reference
    // cells, and the cell_mapping.g2l dictates the conversion
    // from the global numbering to the new local numbering.

    // to the push_comm,
    // the pack map is the cell_mapping.l2g, this is because that
    // the push_comm is responsible to send the dynamic particle
    // cell reference back to their original owners, and these
    // cells are currently in the cell_mapping's local numbering.
    // the unpack map is the loci_g2l because what is received
    // from the push_comm is the cells that belong to a process's
    // original own partition.

    // it is a little confusing partly due to we have no good
    // terms to label those concepts. we may want to work on that
    // later and devise a systematic category of terms that
    // can succinctly and accurately describe these concepts.

    // we will process the dist vector first, which corresponds
    // to the send vector in the push_comm and the recv vector
    // in the pull_comm.

    entitySet alloc = EMPTY ;
    
    for(size_t i=0;i<dist.size();++i) {
      const entitySet& es = dist[i] ;
      if(es != EMPTY) {
        // local numbering for pull.recv[i] & push.send[i]
        entitySet local = remap_entitySet(es, cell_mapping.g2l) ;
        alloc += local ;
        // add to the vectors
        pull.recv.push_back(P2pCommInfo(i, es, local)) ;
        push.send.push_back(P2pCommInfo(i, es, local)) ;
      }
    }
    pull.recv_alloc = alloc ;
    push.send_alloc = alloc ;

    // we then process the dist_t vector, which corresponds to
    // the send vector in the pull_comm and the recv vector
    // in the push_comm
    alloc = EMPTY ;
    for(size_t i=0;i<dist_t.size();++i) {
      const entitySet& es = dist_t[i] ;
      if(es != EMPTY) {
        // local numbering for pull.send[i] & push.recv[i]
        entitySet local = remap_entitySet(es, cell_mapping.loci_g2l) ;
        alloc += local ;
        // add to the vector
        pull.send.push_back(P2pCommInfo(i, es, local)) ;
        push.recv.push_back(P2pCommInfo(i, es, local)) ;
      }
    }
    pull.send_alloc = alloc ;
    push.recv_alloc = alloc ;

    // cache the pack/unpack maps
    pull.pack.setRep(cell_mapping.loci_l2g.Rep()) ;
    pull.unpack.setRep(cell_mapping.g2l.Rep()) ;
    push.pack.setRep(cell_mapping.l2g.Rep()) ;
    push.unpack.setRep(cell_mapping.loci_g2l.Rep()) ;
  }

  void ParticleSpace::
  add_comm(const entitySet& new_domain,
           Communicator& pull, Communicator& push) {
    // we compute two new comms based on the new_domain
    Communicator new_pull, new_push ;
    update_comm(new_domain, new_pull, new_push) ;

    // merge the new comms into the existing ones
    pull += new_pull ;
    push += new_push ;
  }
  
  ////////////////////////////////////////////////////////////////
  // this section implements all the communication routines
  ////////////////////////////////////////////////////////////////

#ifdef TO_BE_REVISED
  // This method is to be revised...
  entitySet
  distributed_map_image_p2p(const entitySet& domain,
                            Loci::MapRepP m, Loci::fact_db* factsP,
                            const std::vector<P2pCommInfo>& send,
                            const std::vector<P2pCommInfo>& recv) {
    
    Loci::fact_db::distribute_infoP df = factsP->get_distribute_info() ;

    // based on the send/recv records, we need to figure out
    // the map image size to send/recv
    
    int total_send_size = 0 ;
    vector<int> send_counts(send.size(), 0) ;

    vector<entitySet> send_image(send.size()) ;
    vector<bool> pack_interval(send.size(), false) ;

    int local_image_copy_idx = -1 ;
    for(size_t i=0;i<send.size();++i) {
      entitySet image = m->image(send[i].entities_l) ;
      image = remap_entitySet(image, df->l2g) ;
      send_image[i] = image ;

      if(send[i].proc == Loci::MPI_rank) {
        // we don't want to send the message to ourselves
        // we can copy that from the local buffer later
        send_counts[i] = 0 ;
        local_image_copy_idx = i ;
        continue ;
      }

      // also if the image is empty, we don't send it
      if(image == EMPTY) {
        send_counts[i] = 0 ;
        continue ;
      }
      
      int num_intervals = image.num_intervals() ;
      int size = image.size() ;
      if(2*num_intervals < size) {
        send_counts[i] = 2*num_intervals + 1 ;
        pack_interval[i] = true ;
      } else {
        pack_interval[i] = false ;
        send_counts[i] = size + 1 ;
      }
      total_send_size += send_counts[i] ;
    }

    vector<int> recv_counts(recv.size(), 0) ;
    vector<MPI_Request> requests(recv_counts.size()) ;

    // recv the size
    size_t req = 0 ;
    for(size_t i=0;i<recv_counts.size();++i) {
      if(recv[i].proc == Loci::MPI_rank) {
        // just record 0 size, we know we are not
        // going to receive from ourselves
        recv_counts[i] = 0 ;
      } else {
        MPI_Irecv(&recv_counts[i], 1, MPI_INT, recv[i].proc,
                  recv[i].proc, MPI_COMM_WORLD, &requests[req++]) ;
      }
    }
    // send the size
    for(size_t i=0;i<send_counts.size();++i) {
      if(send[i].proc != Loci::MPI_rank) {
        MPI_Send(&send_counts[i], 1, MPI_INT,
                 send[i].proc, Loci::MPI_rank, MPI_COMM_WORLD) ;
      }
    }
    // wait all Irecv to finish
    if(req > 0) {
      MPI_Waitall(req, &requests[0], MPI_STATUSES_IGNORE) ;
    }

    vector<int> send_displs(send_counts.size(), 0) ;
    // now we've got the recv size, we prepare the buffer for comm
    if(!send_counts.empty()) {
      send_displs[0] = 0 ;
      for(size_t i=1;i<send_displs.size();++i)
        send_displs[i] = send_displs[i-1] + send_counts[i-1] ;
    }

    int total_recv_size = 0 ;
    vector<int> recv_displs(recv_counts.size(), 0) ;
    if(!recv_counts.empty()) {
      recv_displs[0] = 0 ;
      total_recv_size += recv_counts[0] ;
      for(size_t i=1;i<recv_displs.size();++i) {
        recv_displs[i] = recv_displs[i-1] + recv_counts[i-1] ;
        total_recv_size += recv_counts[i] ;
      }
    }

    // pack the buffer
    vector<int> send_buffer(total_send_size) ;
    int idx = 0 ;
    for(size_t i=0;i<send.size();++i) {
      // don't pack for empty message
      if(send_counts[i] == 0) {
        continue ;
      }
      // image already in global numbering
      const entitySet& image = send_image[i] ;
      // see if pack interval or direct element
      if(pack_interval[i]) {
        send_buffer[idx++] = 1 ; // flag to indicate intervals
        for(int k=0;k<image.num_intervals();++k) {
          send_buffer[idx++] = image[k].first ;
          send_buffer[idx++] = image[k].second ;
        }
        
      } else {
        send_buffer[idx++] = 0 ;
        for(entitySet::const_iterator ei=image.begin();
            ei!=image.end();++ei)
          send_buffer[idx++] = *ei ;
      }
    }

    // prepare the recv buffer
    vector<int> recv_buffer(total_recv_size) ;
    req = 0 ;
    // post recv requests
    for(size_t i=0;i<recv_counts.size();++i) {
      // no need to recv for local message
      // and no need to recv empty message
      if(recv_counts[i] > 0) {
        MPI_Irecv(&recv_buffer[recv_displs[i]],
                  recv_counts[i], MPI_INT, recv[i].proc,
                  recv[i].proc, MPI_COMM_WORLD, &requests[req++]) ;
      }
    }
    // send the buffer
    for(size_t i=0;i<send_counts.size();++i) {
      // only send non-local non-empty message
      if(send_counts[i] > 0) {
        MPI_Send(&send_buffer[send_displs[i]],
                 send_counts[i], MPI_INT, send[i].proc,
                 Loci::MPI_rank, MPI_COMM_WORLD) ;
      }
    }
    // wait all Irecv to finish
    if(req > 0) {
      MPI_Waitall(req, &requests[0], MPI_STATUSES_IGNORE) ;
    }
    // release send buffer
    vector<int>().swap(send_buffer) ;
    // unpack the buffer
    entitySet ret ;

    for(size_t i=0;i<recv.size();++i) {
      int b = recv_displs[i] ;
      int e = b + recv_counts[i] ;
      if(b == e) {
        continue ;              // nothing to extract, continue
      }

      int flag = recv_buffer[b] ;
      ++b ;
      if(flag == 1) {
        // packed with intervals
        for(;b<e;b+=2) {
          int l = recv_buffer[b] ;
          int u = recv_buffer[b+1] ;
          ret += interval(l,u) ;
        }
      } else {
        // packed with elements
        for(;b<e;++b) {
          ret += recv_buffer[b] ;
        }
      }
    }
    if(local_image_copy_idx != -1) {
      // merge the local copy
      ret += send_image[local_image_copy_idx] ;
    }

    return ret ;
  }

  // TO BE REVISED...
  // this is another version of distributed_map_image that uses
  // point 2 point commnunication instead of the collective ones
  entitySet
  distributed_map_image_p2p(const entitySet& domain,
                            Loci::MapRepP m, Loci::fact_db* factsP) {
    // first we compute a communication structure
    // Loci::fact_db::distribute_infoP df = factsP->get_distribute_info() ;

    // ParticleSpace::Communicator comm ;
    // comm.generate_p2p_comm(domain, factsP->get_init_ptn(),
    //                        0 /*no g2l remap allowed*/, (df->l2g).Rep()) ;

    // // now we know the map domain communication structure
    // // then we can call the version with the send/recv struct

    // return distributed_map_image_p2p(domain, m,
    //                                  factsP, comm.send, comm.recv) ;
    return EMPTY ;
  } // end of the function
#endif
  
  void
  get_remote_stores(std::vector<Loci::storeRepP>& in,
                    const std::vector<P2pCommInfo>& send, // the comm
                    const std::vector<P2pCommInfo>& recv, // struct
                    const Map& pack_remap,
                    const dMap& unpack_remap,
                    std::vector<Loci::storeRepP>& out) {
    if(out.size() != in.size())
      out.resize(in.size()) ;
    // first we will need to communicate the size of the send buffer
    // to the receiving process so that they can allocate buffer.
    // we also need to communicate the pack entitySet in sequence
    // to the receiving processes so that they can properly unpack
    // the buffer.
    // normally, we need to send 3 messages, 1) the size of the
    // total packed buffer to send to a particular process, 2)
    // the size of the sequence to send, 3) the sequence itself.
    // in order to save message start-up time, we combine 1) and
    // 2) messages together into one message since they are both
    // integer type.
    vector<int> pack_size(send.size(),0) ;
    vector<int> seq_size(send.size(),0) ;
    vector<bool> pack_interval(send.size(),false) ;
    vector<sequence> pack_seq(send.size()) ;
    // compute pack size first
    for(size_t i=0;i<send.size();++i) {
      for(size_t k=0;k<in.size();++k) {
        pack_size[i] += in[k]->pack_size(send[i].entities_l) ;
      }
    }
    // compute the packing sequence in global numbering
    // and also the way to send the sequence (in intervals
    // or direct elements), and the sequence send size.
    for(size_t i=0;i<send.size();++i) {
      pack_seq[i] = remap_sequence(sequence(send[i].entities_l),
                                   pack_remap) ;
      int interval_size = pack_seq[i].num_intervals() ;
      int elem_size = pack_seq[i].size() ;
      if(2*interval_size < elem_size) {
        pack_interval[i] = true ;
        seq_size[i] = 2*interval_size + 1 ;
      } else {
        pack_interval[i] = false ;
        seq_size[i] = elem_size + 1 ;
      }
    }
    // now send the total pack size and seq size first
    vector<int> recv_msg_size(recv.size()*2,0) ;

    vector<MPI_Request> requests(recv.size()) ;
    // first post recv requests
    int req = 0 ;
    // this is used to optimize in the case of sending/receiving
    // messages to itself, we instead would just do a copy
    int self_msg_buffer_idx = -1 ;
    for(size_t i=0;i<recv.size();++i) {
      if(recv[i].proc == Loci::MPI_rank) {
        self_msg_buffer_idx = i ;
      } else {
        MPI_Irecv(&recv_msg_size[i*2], 2, MPI_INT, recv[i].proc,
                  recv[i].proc, MPI_COMM_WORLD, &requests[req++]) ;
      }
    }
    // then post send requests
    for(size_t i=0;i<send.size();++i) {
      if(send[i].proc == Loci::MPI_rank) {
        // just do a copy
        recv_msg_size[2*self_msg_buffer_idx] = pack_size[i] ;
        // this is another optimization, we do not need to
        // communicate the packing sequence for myself
        recv_msg_size[2*self_msg_buffer_idx+1] = 0 ;
      } else {
        int tmp[2] ;
        // first one is the total pack size
        tmp[0] = pack_size[i] ;
        // second one is the pack sequence size
        tmp[1] = seq_size[i] ;
        MPI_Send(tmp, 2, MPI_INT,
                 send[i].proc, Loci::MPI_rank, MPI_COMM_WORLD) ;
      }
    }
    // wait all Irecv to finish
    if(req > 0) {
      MPI_Waitall(req, &requests[0], MPI_STATUSES_IGNORE) ;
    }
    // then we actually need to communicate the packing sequence
    // to all the receiving processes

    // allocate recv buffer first
    int total_recv_size = 0 ;
    for(size_t i=0;i<recv.size();++i)
      total_recv_size += recv_msg_size[2*i+1] ;
    int* unpack_seq_recv_buffer = new int[total_recv_size] ;
    int** unpack_seq_recv_buffer_ptr = new int*[recv.size()] ;

    if(!recv.empty()) {
      unpack_seq_recv_buffer_ptr[0] = unpack_seq_recv_buffer ;
      for(size_t i=1;i<recv.size();++i)
        unpack_seq_recv_buffer_ptr[i] = recv_msg_size[2*(i-1)+1] +
          unpack_seq_recv_buffer_ptr[i-1] ;
    }
    
    // post recv requests
    req = 0 ;
    for(size_t i=0;i<recv.size();++i) {
      if(recv[i].proc != Loci::MPI_rank) {
        MPI_Irecv(unpack_seq_recv_buffer_ptr[i],
                  recv_msg_size[2*i+1],
                  MPI_INT, recv[i].proc,
                  recv[i].proc, MPI_COMM_WORLD, &requests[req++]) ;
      }
    }
    // do the send
    for(size_t i=0;i<send.size();++i) {
      if(send[i].proc != Loci::MPI_rank) {
        // allocate a send buffer
        vector<int> buf(seq_size[i]) ;
        // pack it
        if(pack_interval[i]) {
          // pack intervals
          buf[0] = 1 ;
          int count = 1 ;
          const sequence& seq = pack_seq[i] ;
          for(size_t k=0;k<seq.num_intervals();++k) {
            buf[count++] = seq[k].first ;
            buf[count++] = seq[k].second ;
          }
        } else {
          // pack elements
          buf[0] = 0 ;
          int count = 1 ;
          const sequence& seq = pack_seq[i] ;
          for(sequence::const_iterator si=seq.begin();
              si!=seq.end();++si,++count)
            buf[count] = *si ;
        }
        // send the buffer
        MPI_Send(&buf[0], seq_size[i], MPI_INT,
                 send[i].proc, Loci::MPI_rank, MPI_COMM_WORLD) ;
      } else {
        self_msg_buffer_idx = i ;
      }
    }
    // wait all Irecv to finish
    if(req > 0) {
      MPI_Waitall(req, &requests[0], MPI_STATUSES_IGNORE) ;
    }
    // then unpack the sequence buffer
    vector<sequence> unpack_seq(recv.size()) ;
    for(size_t i=0;i<recv.size();++i) {
      if(recv[i].proc == Loci::MPI_rank) {
        // just copy
        unpack_seq[i] = pack_seq[self_msg_buffer_idx] ;
      } else {
        // extract the first int to see if the packed
        // stuff is interval or element
        sequence& seq = unpack_seq[i] ;
        int* p = unpack_seq_recv_buffer_ptr[i] ;
        int size = recv_msg_size[2*i+1] - 1 ;
        if(*p == 1) {
          // extract intervals
          ++p ;
          for(int k=0;k<size;k+=2) {
            int b = *p ; ++p ;
            int e = *p ; ++p ;
            seq += interval(b,e) ;
          }
        } else {
          // extract elements
          ++p ;
          for(int k=0;k<size;++k,++p)
            seq += *p ;
        }
      }
    }
    // release all un-necessary buffers
    vector<int>().swap(seq_size) ;
    vector<bool>().swap(pack_interval) ;
    vector<sequence>().swap(pack_seq) ;
    delete[] unpack_seq_recv_buffer_ptr ;
    delete[] unpack_seq_recv_buffer ;

    // remap the unpack sequence to the new numbering
    for(size_t i=0;i<recv.size();++i) {
      unpack_seq[i] = remap_sequence(unpack_seq[i], unpack_remap) ;
    }
    
    // now we have enough information to send/recv the real
    // contents in the storeRepP

    // allocate a recv buffer first.
    total_recv_size = 0 ;
    for(size_t i=0;i<recv.size();++i)
      total_recv_size += recv_msg_size[2*i] ; // the unpack size for i
    unsigned char* unpack_buffer = new unsigned char[total_recv_size] ;
    unsigned char** unpack_buffer_ptr = new unsigned char*[recv.size()] ;

    if(!recv.empty()) {
      unpack_buffer_ptr[0] = unpack_buffer ;
      for(size_t i=1;i<recv.size();++i)
        unpack_buffer_ptr[i] = recv_msg_size[2*(i-1)] +
          unpack_buffer_ptr[i-1] ;
    }
    // post recv requests
    req = 0 ;
    for(size_t i=0;i<recv.size();++i) {
      if(recv[i].proc == Loci::MPI_rank) {
        self_msg_buffer_idx = i ;
      } else {
        MPI_Irecv(unpack_buffer_ptr[i], recv_msg_size[2*i],
                  MPI_PACKED, recv[i].proc, recv[i].proc,
                  MPI_COMM_WORLD, &requests[req++]) ;
      }
    }
    // actually pack and send
    for(size_t i=0;i<send.size();++i) {
      if(send[i].proc == Loci::MPI_rank) {
        unsigned char* pack_buffer =
          unpack_buffer_ptr[self_msg_buffer_idx] ;
        int position = 0 ;
        for(size_t k=0;k<in.size();++k) {
          in[k]->pack(pack_buffer, position,
                      pack_size[i], send[i].entities_l) ;
        }
        
      } else {
        // first allocate the pack buffer
        unsigned char* pack_buffer = new unsigned char[pack_size[i]] ;
        // then do the pack
        int position = 0 ;
        for(size_t k=0;k<in.size();++k) {
          // pack the contents
          in[k]->pack(pack_buffer, position,
                      pack_size[i], send[i].entities_l) ;
        }
        // done all the packing, send it
        MPI_Send(pack_buffer, pack_size[i], MPI_PACKED,
                 send[i].proc, Loci::MPI_rank, MPI_COMM_WORLD) ;
        // destroy the send buffer
        delete[] pack_buffer ;
      }
    }
    // wait all Irecv to finish
    if(req > 0) {
      MPI_Waitall(req, &requests[0], MPI_STATUSES_IGNORE) ;
    }

    // then unpack the receiving buffer

    // first compute the new domain for all the out storeRepP
    entitySet new_domain ;
    for(size_t i=0;i<recv.size();++i) {
      new_domain += recv[i].entities_g ;
    }
    // remap it to the new numbering
    new_domain = remap_entitySet(new_domain, unpack_remap) ;
    // allocate all storeRepP in out
    for(size_t i=0;i<out.size();++i) {
      out[i] = in[i]->new_store(new_domain) ;
    }

    // and finally we will unpack the buffer
    for(size_t i=0;i<recv.size();++i) {
      int position = 0 ;
      int unpack_buffer_size = recv_msg_size[2*i] ;
      for(size_t k=0;k<out.size();++k) {
        // unpack the contents
        out[k]->unpack(unpack_buffer_ptr[i], position,
                       unpack_buffer_size, unpack_seq[i]) ;
      }
    }
    // release recv buffer and finish up
    delete[] unpack_buffer_ptr ;
    delete[] unpack_buffer ;
  }

  // NOTE: we don't check if the remap contained in the
  // comm is valid (e.g., not having a null Rep, have a correct
  // domain, etc.). it is user's responsibility to ensure
  // that the "Communicator" passed is compatible for this
  // function.
  void
  get_remote_stores(std::vector<Loci::storeRepP>& in,
                    std::vector<Loci::storeRepP>& out,
                    const ParticleSpace::Communicator& comm) {
    // extract the mapping info from the comm structure
    Map pack_remap ; pack_remap.setRep(comm.pack.Rep()) ;
    dMap unpack_remap ; unpack_remap.setRep(comm.unpack.Rep()) ;

    get_remote_stores(in, comm.send, comm.recv,
                      pack_remap, unpack_remap, out) ;
  }
  
  Loci::storeRepP
  get_remote_stores(Loci::storeRepP in,
                    const std::vector<P2pCommInfo>& send,
                    const std::vector<P2pCommInfo>& recv,
                    const Map& pack_remap,
                    const dMap& unpack_remap) {
    vector<Loci::storeRepP> vs(1), out(1) ;
    vs[0] = in ;
    get_remote_stores(vs,send,recv,pack_remap,unpack_remap,out) ;
    return out[0] ;
  }

  Loci::storeRepP
  get_remote_stores(Loci::storeRepP in,
                    const ParticleSpace::Communicator& comm) {
    // extract the mapping info from the comm structure
    Map pack_remap ; pack_remap.setRep(comm.pack.Rep()) ;
    dMap unpack_remap ; unpack_remap.setRep(comm.unpack.Rep()) ;

    return get_remote_stores(in, comm.send, comm.recv,
                             pack_remap, unpack_remap) ;
  }

  void
  get_remote_stores_inc(std::vector<Loci::storeRepP>& in,
                        const std::vector<P2pCommInfo>& send,
                        const std::vector<P2pCommInfo>& recv,
                        const Map& pack_remap,
                        const dMap& unpack_remap,
                        std::vector<Loci::storeRepP>& out) {
    // first we will need to communicate the size of the send buffer
    // to the receiving process so that they can allocate buffer.
    // we also need to communicate the pack entitySet in sequence
    // to the receiving processes so that they can properly unpack
    // the buffer.
    // normally, we need to send 3 messages, 1) the size of the
    // total packed buffer to send to a particular process, 2)
    // the size of the sequence to send, 3) the sequence itself.
    // in order to save message start-up time, we combine 1) and
    // 2) messages together into one message since they are both
    // integer type.
    vector<int> pack_size(send.size(),0) ;
    vector<int> seq_size(send.size(),0) ;
    vector<bool> pack_interval(send.size(),false) ;
    vector<sequence> pack_seq(send.size()) ;
    // compute pack size first
    for(size_t i=0;i<send.size();++i) {
      for(size_t k=0;k<in.size();++k) {
        pack_size[i] += in[k]->pack_size(send[i].entities_l) ;
      }
    }
    // compute the packing sequence in global numbering
    // and also the way to send the sequence (in intervals
    // or direct elements), and the sequence send size.
    for(size_t i=0;i<send.size();++i) {
      pack_seq[i] = remap_sequence(sequence(send[i].entities_l),
                                   pack_remap) ;
      int interval_size = pack_seq[i].num_intervals() ;
      int elem_size = pack_seq[i].size() ;
      if(2*interval_size < elem_size) {
        pack_interval[i] = true ;
        seq_size[i] = 2*interval_size + 1 ;
      } else {
        pack_interval[i] = false ;
        seq_size[i] = elem_size + 1 ;
      }
    }
    // now send the total pack size and seq size first
    vector<int> recv_msg_size(recv.size()*2,0) ;

    vector<MPI_Request> requests(recv.size()) ;
    // first post recv requests
    int req = 0 ;
    // this is used to optimize in the case of sending/receiving
    // messages to itself, we instead would just do a copy
    int self_msg_buffer_idx = -1 ;
    for(size_t i=0;i<recv.size();++i) {
      if(recv[i].proc == Loci::MPI_rank) {
        self_msg_buffer_idx = i ;
      } else {
        MPI_Irecv(&recv_msg_size[i*2], 2, MPI_INT, recv[i].proc,
                  recv[i].proc, MPI_COMM_WORLD, &requests[req++]) ;
      }
    }
    // then post send requests
    for(size_t i=0;i<send.size();++i) {
      if(send[i].proc == Loci::MPI_rank) {
        // just do a copy
        recv_msg_size[2*self_msg_buffer_idx] = pack_size[i] ;
        // this is another optimization, we do not need to
        // communicate the packing sequence for myself
        recv_msg_size[2*self_msg_buffer_idx+1] = 0 ;
      } else {
        int tmp[2] ;
        // first one is the total pack size
        tmp[0] = pack_size[i] ;
        // second one is the pack sequence size
        tmp[1] = seq_size[i] ;
        MPI_Send(tmp, 2, MPI_INT,
                 send[i].proc, Loci::MPI_rank, MPI_COMM_WORLD) ;
      }
    }
    // wait all Irecv to finish
    if(req > 0) {
      MPI_Waitall(req, &requests[0], MPI_STATUSES_IGNORE) ;
    }
    // then we actually need to communicate the packing sequence
    // to all the receiving processes

    // allocate recv buffer first
    int total_recv_size = 0 ;
    for(size_t i=0;i<recv.size();++i)
      total_recv_size += recv_msg_size[2*i+1] ;

    int* unpack_seq_recv_buffer = new int[total_recv_size] ;
    int** unpack_seq_recv_buffer_ptr = new int*[recv.size()] ;

    if(!recv.empty()) {
      unpack_seq_recv_buffer_ptr[0] = unpack_seq_recv_buffer ;
      for(size_t i=1;i<recv.size();++i)
        unpack_seq_recv_buffer_ptr[i] = recv_msg_size[2*(i-1)+1] +
          unpack_seq_recv_buffer_ptr[i-1] ;
    }
    
    // post recv requests
    req = 0 ;
    for(size_t i=0;i<recv.size();++i) {
      if(recv[i].proc != Loci::MPI_rank) {
        MPI_Irecv(unpack_seq_recv_buffer_ptr[i],
                  recv_msg_size[2*i+1],
                  MPI_INT, recv[i].proc,
                  recv[i].proc, MPI_COMM_WORLD, &requests[req++]) ;
      }
    }
    // do the send
    for(size_t i=0;i<send.size();++i) {
      if(send[i].proc != Loci::MPI_rank) {
        // allocate a send buffer
        vector<int> buf(seq_size[i]) ;
        // pack it
        if(pack_interval[i]) {
          // pack intervals
          buf[0] = 1 ;
          int count = 1 ;
          const sequence& seq = pack_seq[i] ;
          for(size_t k=0;k<seq.num_intervals();++k) {
            buf[count++] = seq[k].first ;
            buf[count++] = seq[k].second ;
          }
        } else {
          // pack elements
          buf[0] = 0 ;
          int count = 1 ;
          const sequence& seq = pack_seq[i] ;
          for(sequence::const_iterator si=seq.begin();
              si!=seq.end();++si,++count)
            buf[count] = *si ;
        }
        // send the buffer
        MPI_Send(&buf[0], seq_size[i], MPI_INT,
                 send[i].proc, Loci::MPI_rank, MPI_COMM_WORLD) ;
      } else {
        self_msg_buffer_idx = i ;
      }
    }
    // wait all Irecv to finish
    if(req > 0) {
      MPI_Waitall(req, &requests[0], MPI_STATUSES_IGNORE) ;
    }
    // then unpack the sequence buffer
    vector<sequence> unpack_seq(recv.size()) ;
    for(size_t i=0;i<recv.size();++i) {
      if(recv[i].proc == Loci::MPI_rank) {
        // just copy
        unpack_seq[i] = pack_seq[self_msg_buffer_idx] ;
      } else {
        // extract the first int to see if the packed
        // stuff is interval or element
        sequence& seq = unpack_seq[i] ;
        int* p = unpack_seq_recv_buffer_ptr[i] ;
        int size = recv_msg_size[2*i+1] - 1 ;
        if(*p == 1) {
          // extract intervals
          ++p ;
          for(int k=0;k<size;k+=2) {
            int b = *p ; ++p ;
            int e = *p ; ++p ;
            seq += interval(b,e) ;
          }
        } else {
          // extract elements
          ++p ;
          for(int k=0;k<size;++k,++p)
            seq += *p ;
        }
      }
    }
    // release all un-necessary buffers
    vector<int>().swap(seq_size) ;
    vector<bool>().swap(pack_interval) ;
    vector<sequence>().swap(pack_seq) ;
    delete[] unpack_seq_recv_buffer_ptr ;
    delete[] unpack_seq_recv_buffer ;

    // remap the unpack sequence to the new numbering
    for(size_t i=0;i<recv.size();++i) {
      unpack_seq[i] = remap_sequence(unpack_seq[i], unpack_remap) ;
    }
    
    // now we have enough information to send/recv the real
    // contents in the storeRepP

    // allocate a recv buffer first.
    total_recv_size = 0 ;
    for(size_t i=0;i<recv.size();++i)
      total_recv_size += recv_msg_size[2*i] ; // the unpack size for i

    unsigned char* unpack_buffer = new unsigned char[total_recv_size] ;
    unsigned char** unpack_buffer_ptr = new unsigned char*[recv.size()] ;

    if(!recv.empty()) {
      unpack_buffer_ptr[0] = unpack_buffer ;
      for(size_t i=1;i<recv.size();++i)
        unpack_buffer_ptr[i] = recv_msg_size[2*(i-1)] +
          unpack_buffer_ptr[i-1] ;
    }
    // post recv requests
    req = 0 ;
    for(size_t i=0;i<recv.size();++i) {
      if(recv[i].proc == Loci::MPI_rank) {
        self_msg_buffer_idx = i ;
      } else {
        MPI_Irecv(unpack_buffer_ptr[i], recv_msg_size[2*i],
                  MPI_PACKED, recv[i].proc, recv[i].proc,
                  MPI_COMM_WORLD, &requests[req++]) ;
      }
    }
    // actually pack and send
    for(size_t i=0;i<send.size();++i) {
      if(send[i].proc == Loci::MPI_rank) {
        unsigned char* pack_buffer =
          unpack_buffer_ptr[self_msg_buffer_idx] ;
        int position = 0 ;
        for(size_t k=0;k<in.size();++k) {
          in[k]->pack(pack_buffer, position,
                      pack_size[i], send[i].entities_l) ;
        }
        
      } else {
        // first allocate the pack buffer
        unsigned char* pack_buffer = new unsigned char[pack_size[i]] ;
        // then do the pack
        int position = 0 ;
        for(size_t k=0;k<in.size();++k) {
          // pack the contents
          in[k]->pack(pack_buffer, position,
                      pack_size[i], send[i].entities_l) ;
        }
        // done all the packing, send it
        MPI_Send(pack_buffer, pack_size[i], MPI_PACKED,
                 send[i].proc, Loci::MPI_rank, MPI_COMM_WORLD) ;
        // destroy the send buffer
        delete[] pack_buffer ;
      }
    }
    // wait all Irecv to finish
    if(req > 0) {
      MPI_Waitall(req, &requests[0], MPI_STATUSES_IGNORE) ;
    }

    // then unpack the receiving buffer
    for(size_t i=0;i<recv.size();++i) {
      int position = 0 ;
      int unpack_buffer_size = recv_msg_size[2*i] ;
      for(size_t k=0;k<out.size();++k) {
        // unpack the contents
        out[k]->unpack(unpack_buffer_ptr[i], position,
                       unpack_buffer_size, unpack_seq[i]) ;
      }
    }
    // release recv buffer and finish up
    delete[] unpack_buffer_ptr ;
    delete[] unpack_buffer ;
  }

  void
  get_remote_stores_inc(std::vector<Loci::storeRepP>& in,
                        std::vector<Loci::storeRepP>& out,
                        const ParticleSpace::Communicator& comm) {
    // extract the mapping info from the comm structure
    Map pack_remap ; pack_remap.setRep(comm.pack.Rep()) ;
    dMap unpack_remap ; unpack_remap.setRep(comm.unpack.Rep()) ;

    get_remote_stores_inc(in, comm.send, comm.recv,
                          pack_remap, unpack_remap, out) ;
  }

  // this method is mostly the same as the "get_remote_stores" one,
  // just that this is a specific version applied to maps in Loci.
  // In commnicuating maps, we will need to have more remapping
  // options, specifically the domain the map can be remapped before/afer
  // the communication, also the image of the map can be remapped as well.
  // In addition, this method also works for dynamic maps (dMap) where
  // such remapping might not be desirable.  In this case, if the
  // remapping info are NULL pointers, then we just don't do remapping
  void
  get_remote_maps(std::vector<Loci::storeRepP>& in,
                  const std::vector<P2pCommInfo>& send, // the comm
                  const std::vector<P2pCommInfo>& recv, // struct
                  Map* dom_pack, // remapping info for packing domains
                  dMap* dom_unpack, // remapping info for unpacking domains
                  Map* img_pack, // remapping info for packing images
                  dMap* img_unpack, // remapping infor for unpacking images
                  std::vector<Loci::storeRepP>& out) {
    if(out.size() != in.size())
      out.resize(in.size()) ;
    // first we will need to communicate the size of the send buffer
    // to the receiving process so that they can allocate buffer.
    // we also need to communicate the pack entitySet in sequence
    // to the receiving processes so that they can properly unpack
    // the buffer.
    // normally, we need to send 3 messages, 1) the size of the
    // total packed buffer to send to a particular process, 2)
    // the size of the sequence to send, 3) the sequence itself.
    // in order to save message start-up time, we combine 1) and
    // 2) messages together into one message since they are both
    // integer type.
    vector<int> pack_size(send.size(),0) ;
    vector<int> seq_size(send.size(),0) ;
    vector<bool> pack_interval(send.size(),false) ;
    vector<sequence> pack_seq(send.size()) ;
    // compute pack size first
    for(size_t i=0;i<send.size();++i) {
      for(size_t k=0;k<in.size();++k) {
        pack_size[i] += in[k]->pack_size(send[i].entities_l) ;
      }
    }
    // compute the packing sequence and also the way to send the
    // sequence (in intervals or direct elements), and the
    // sequence send size.
    for(size_t i=0;i<send.size();++i) {
      pack_seq[i] = sequence(send[i].entities_l);
      // remapping the sequence if necessary
      if(dom_pack)
        pack_seq[i] = remap_sequence(pack_seq[i], *dom_pack) ;
      
      int interval_size = pack_seq[i].num_intervals() ;
      int elem_size = pack_seq[i].size() ;
      if(2*interval_size < elem_size) {
        pack_interval[i] = true ;
        seq_size[i] = 2*interval_size + 1 ;
      } else {
        pack_interval[i] = false ;
        seq_size[i] = elem_size + 1 ;
      }
    }
    // now send the total pack size and seq size first
    vector<int> recv_msg_size(recv.size()*2,0) ;

    vector<MPI_Request> requests(recv.size()) ;
    // first post recv requests
    int req = 0 ;
    // this is used to optimize in the case of sending/receiving
    // messages to itself, we instead would just do a copy
    int self_msg_buffer_idx = -1 ;
    for(size_t i=0;i<recv.size();++i) {
      if(recv[i].proc == Loci::MPI_rank) {
        self_msg_buffer_idx = i ;
      } else {
        MPI_Irecv(&recv_msg_size[i*2], 2, MPI_INT, recv[i].proc,
                  recv[i].proc, MPI_COMM_WORLD, &requests[req++]) ;
      }
    }
    // then post send requests
    for(size_t i=0;i<send.size();++i) {
      if(send[i].proc == Loci::MPI_rank) {
        // just do a copy
        recv_msg_size[2*self_msg_buffer_idx] = pack_size[i] ;
        // this is another optimization, we do not need to
        // communicate the packing sequence for myself
        recv_msg_size[2*self_msg_buffer_idx+1] = 0 ;
      } else {
        int tmp[2] ;
        // first one is the total pack size
        tmp[0] = pack_size[i] ;
        // second one is the pack sequence size
        tmp[1] = seq_size[i] ;
        MPI_Send(tmp, 2, MPI_INT,
                 send[i].proc, Loci::MPI_rank, MPI_COMM_WORLD) ;
      }
    }
    // wait all Irecv to finish
    if(req > 0) {
      MPI_Waitall(req, &requests[0], MPI_STATUSES_IGNORE) ;
    }

    // then we actually need to communicate the packing sequence
    // to all the receiving processes

    // allocate recv buffer first
    int total_recv_size = 0 ;
    for(size_t i=0;i<recv.size();++i)
      total_recv_size += recv_msg_size[2*i+1] ;
    int* unpack_seq_recv_buffer = new int[total_recv_size] ;
    int** unpack_seq_recv_buffer_ptr = new int*[recv.size()] ;

    if(!recv.empty()) {
      unpack_seq_recv_buffer_ptr[0] = unpack_seq_recv_buffer ;
      for(size_t i=1;i<recv.size();++i)
        unpack_seq_recv_buffer_ptr[i] = recv_msg_size[2*(i-1)+1] +
          unpack_seq_recv_buffer_ptr[i-1] ;
    }
    
    // post recv requests
    req = 0 ;
    for(size_t i=0;i<recv.size();++i) {
      if(recv[i].proc != Loci::MPI_rank) {
        MPI_Irecv(unpack_seq_recv_buffer_ptr[i],
                  recv_msg_size[2*i+1],
                  MPI_INT, recv[i].proc,
                  recv[i].proc, MPI_COMM_WORLD, &requests[req++]) ;
      }
    }
    // do the send
    for(size_t i=0;i<send.size();++i) {
      if(send[i].proc != Loci::MPI_rank) {
        // allocate a send buffer
        vector<int> buf(seq_size[i]) ;
        // pack it
        if(pack_interval[i]) {
          // pack intervals
          buf[0] = 1 ;
          int count = 1 ;
          const sequence& seq = pack_seq[i] ;
          for(size_t k=0;k<seq.num_intervals();++k) {
            buf[count++] = seq[k].first ;
            buf[count++] = seq[k].second ;
          }
        } else {
          // pack elements
          buf[0] = 0 ;
          int count = 1 ;
          const sequence& seq = pack_seq[i] ;
          for(sequence::const_iterator si=seq.begin();
              si!=seq.end();++si,++count)
            buf[count] = *si ;
        }
        // send the buffer
        MPI_Send(&buf[0], seq_size[i], MPI_INT,
                 send[i].proc, Loci::MPI_rank, MPI_COMM_WORLD) ;
      } else {
        self_msg_buffer_idx = i ;
      }
    }
    // wait all Irecv to finish
    if(req > 0) {
      MPI_Waitall(req, &requests[0], MPI_STATUSES_IGNORE) ;
    }

    // then unpack the sequence buffer
    vector<sequence> unpack_seq(recv.size()) ;
    for(size_t i=0;i<recv.size();++i) {
      if(recv[i].proc == Loci::MPI_rank) {
        // just copy
        unpack_seq[i] = pack_seq[self_msg_buffer_idx] ;
      } else {
        // extract the first int to see if the packed
        // stuff is interval or element
        sequence& seq = unpack_seq[i] ;
        int* p = unpack_seq_recv_buffer_ptr[i] ;
        int size = recv_msg_size[2*i+1] - 1 ;
        if(*p == 1) {
          // extract intervals
          ++p ;
          for(int k=0;k<size;k+=2) {
            int b = *p ; ++p ;
            int e = *p ; ++p ;
            seq += interval(b,e) ;
          }
        } else {
          // extract elements
          ++p ;
          for(int k=0;k<size;++k,++p)
            seq += *p ;
        }
      }
    }
    // release all un-necessary buffers
    vector<int>().swap(seq_size) ;
    vector<bool>().swap(pack_interval) ;
    vector<sequence>().swap(pack_seq) ;
    delete[] unpack_seq_recv_buffer_ptr ;
    delete[] unpack_seq_recv_buffer ;

    // remap the unpack sequence to the new numbering if necessary
    if(dom_unpack) {
      for(size_t i=0;i<recv.size();++i) {
        unpack_seq[i] = remap_sequence(unpack_seq[i], *dom_unpack) ;
      }
    }
    
    // now we have enough information to send/recv the real
    // contents in the storeRepP

    // allocate a recv buffer first.
    total_recv_size = 0 ;
    for(size_t i=0;i<recv.size();++i)
      total_recv_size += recv_msg_size[2*i] ; // the unpack size for i
    unsigned char* unpack_buffer = new unsigned char[total_recv_size] ;
    unsigned char** unpack_buffer_ptr = new unsigned char*[recv.size()] ;

    if(!recv.empty()) {
      unpack_buffer_ptr[0] = unpack_buffer ;
      for(size_t i=1;i<recv.size();++i)
        unpack_buffer_ptr[i] = recv_msg_size[2*(i-1)] +
          unpack_buffer_ptr[i-1] ;
    }
    // post recv requests
    req = 0 ;
    for(size_t i=0;i<recv.size();++i) {
      if(recv[i].proc == Loci::MPI_rank) {
        self_msg_buffer_idx = i ;
      } else {
        MPI_Irecv(unpack_buffer_ptr[i], recv_msg_size[2*i],
                  MPI_PACKED, recv[i].proc, recv[i].proc,
                  MPI_COMM_WORLD, &requests[req++]) ;
      }
    }
    // actually pack and send
    for(size_t i=0;i<send.size();++i) {
      if(send[i].proc == Loci::MPI_rank) {
        unsigned char* pack_buffer =
          unpack_buffer_ptr[self_msg_buffer_idx] ;
        int position = 0 ;
        // we need to remap the images when necessary, using the
        // special pack function in the map class that also does remap
        if(img_pack) {
          for(size_t k=0;k<in.size();++k) {
            in[k]->pack(pack_buffer, position,
                        pack_size[i], send[i].entities_l, *img_pack) ;
          }
        } else {
          for(size_t k=0;k<in.size();++k) {
            in[k]->pack(pack_buffer, position,
                        pack_size[i], send[i].entities_l) ;
          }
        }
        
      } else {
        // first allocate the pack buffer
        unsigned char* pack_buffer = new unsigned char[pack_size[i]] ;
        // then do the pack
        int position = 0 ;
        // remapping the images when necessary
        if(img_pack) {
          for(size_t k=0;k<in.size();++k) {
            // pack the contents
            in[k]->pack(pack_buffer, position,
                        pack_size[i], send[i].entities_l, *img_pack) ;
          }
        } else {
          for(size_t k=0;k<in.size();++k) {
            in[k]->pack(pack_buffer, position,
                        pack_size[i], send[i].entities_l) ;
          }
        }
        // done all the packing, send it
        MPI_Send(pack_buffer, pack_size[i], MPI_PACKED,
                 send[i].proc, Loci::MPI_rank, MPI_COMM_WORLD) ;
        // destroy the send buffer
        delete[] pack_buffer ;
      }
    }
    // wait all Irecv to finish
    if(req > 0) {
      MPI_Waitall(req, &requests[0], MPI_STATUSES_IGNORE) ;
    }

    // then unpack the receiving buffer

    // first compute the new domain for all the out MapRepP
    entitySet new_domain ;
    for(size_t i=0;i<recv.size();++i) {
      new_domain += recv[i].entities_g ;
    }
    // remap it to the new numbering if needed
    if(dom_unpack)
      new_domain = remap_entitySet(new_domain, *dom_unpack) ;
    // allocate all MapRepP in out
    for(size_t i=0;i<out.size();++i) {
      out[i] = in[i]->new_store(new_domain) ;
      //out[i]->allocate(new_domain);
    }

    // and finally we will unpack the buffer
    for(size_t i=0;i<recv.size();++i) {
      int position = 0 ;
      int unpack_buffer_size = recv_msg_size[2*i] ;
      if(img_unpack) {
        for(size_t k=0;k<out.size();++k) {
          // unpack the contents
          out[k]->unpack(unpack_buffer_ptr[i], position,
                         unpack_buffer_size, unpack_seq[i], *img_unpack) ;
        }
      } else {
        for(size_t k=0;k<out.size();++k) {
          out[k]->unpack(unpack_buffer_ptr[i], position,
                         unpack_buffer_size, unpack_seq[i]) ;
        }
      }
    }

    // release recv buffer and finish up
    delete[] unpack_buffer_ptr ;
    delete[] unpack_buffer ;
  }


#ifdef TO_BE_REVISED
  // These methods are to be revised...
  Loci::storeRepP
  get_remote_Map(const Map& in,
                 const std::vector<P2pCommInfo>& send,
                 const std::vector<P2pCommInfo>& recv,
                 const dMap& remap, Loci::fact_db* factsP) {
    Loci::fact_db::distribute_infoP df = factsP->get_distribute_info() ;  
    // first we will need to communicate the size of the send buffer
    int total_send_size = 0 ;
    vector<int> send_counts(send.size(),0) ;
    for(size_t i=0;i<send.size();++i) {
      const entitySet& domain = send[i].entities_l ;
      send_counts[i] = 2 * domain.size() ;
      total_send_size += send_counts[i] ;
    }

    vector<int> recv_counts(recv.size(),0) ;
    vector<MPI_Request> requests(recv.size()) ;

    // recv the size
    size_t req = 0 ;
    int local_offset = 0 ;
    for(size_t i=0;i<recv.size();++i) {
      if(recv[i].proc == Loci::MPI_rank) {
        local_offset = i ;
      } else {
        MPI_Irecv(&recv_counts[i], 1, MPI_INT, recv[i].proc,
                  recv[i].proc, MPI_COMM_WORLD, &requests[req++]) ;
      }
    }
    // send the size
    for(size_t i=0;i<send.size();++i) {
      if(send[i].proc == Loci::MPI_rank) {
        recv_counts[local_offset] = send_counts[i] ;
      } else {
        MPI_Send(&send_counts[i], 1, MPI_INT,
                 send[i].proc, Loci::MPI_rank, MPI_COMM_WORLD) ;
      }
    }
    // wait all Irecv
    if(req > 0) {
      MPI_Waitall(req, &requests[0], MPI_STATUSES_IGNORE) ;
    }

    vector<int> send_displs(send.size(), 0) ;
    if(!send.empty()) {
      send_displs[0] = 0 ;
      for(size_t i=1;i<send.size();++i)
        send_displs[i] = send_displs[i-1] + send_counts[i-1] ;
    }

    int total_recv_size = 0 ;
    vector<int> recv_displs(recv.size(), 0) ;
    if(!recv.empty()) {
      recv_displs[0] = 0 ;
      total_recv_size += recv_counts[0] ;
      for(size_t i=1;i<recv.size();++i) {
        recv_displs[i] = recv_displs[i-1] + recv_counts[i-1] ;
        total_recv_size += recv_counts[i] ;
      }
    }

    // pack the buffer
    vector<int> send_buffer(total_send_size) ;
    int idx = 0 ;
    for(size_t i=0;i<send.size();++i) {
      const entitySet& domain = send[i].entities_l ;
      for(entitySet::const_iterator ei=domain.begin();
          ei!=domain.end();++ei) {
        // first store the domain entity (in global numbering)
        send_buffer[idx++] = df->l2g[*ei] ;
        // elements also in global numbering
        send_buffer[idx++] = df->l2g[in[*ei]] ;
      }
    }

    // allocate recv buffer
    vector<int> recv_buffer(total_recv_size) ;
    req = 0 ;
    // recv
    for(size_t i=0;i<recv.size();++i) {
      if(recv[i].proc == Loci::MPI_rank) {
        local_offset = recv_displs[i] ;
      } else {
        MPI_Irecv(&recv_buffer[recv_displs[i]],
                  recv_counts[i], MPI_INT, recv[i].proc,
                  recv[i].proc, MPI_COMM_WORLD, &requests[req++]) ;
      }
    }
    // send the buffer
    for(size_t i=0;i<send.size();++i) {
      if(send[i].proc == Loci::MPI_rank) {
        // do a memory copy instead of send
        for(int k=send_displs[i];k<send_displs[i]+send_counts[i];
            ++k,++local_offset)
          recv_buffer[local_offset] = send_buffer[k] ;
      } else {
        MPI_Send(&send_buffer[send_displs[i]],
                 send_counts[i], MPI_INT, send[i].proc,
                 Loci::MPI_rank, MPI_COMM_WORLD) ;
      }
    }
    // wait all Irecv
    if(req > 0) {
      MPI_Waitall(req, &requests[0], MPI_STATUSES_IGNORE) ;
    }
    // release the useless buffer
    vector<int>().swap(send_buffer) ;
    vector<int>().swap(send_counts) ;
    vector<int>().swap(send_displs) ;

    // extract the buffer
    // get the new domain first
    entitySet new_domain ;
    for(size_t i=0;i<recv.size();++i)
      new_domain += recv[i].entities_g ;
    // remap to new numbering
    new_domain = remap_entitySet(new_domain, remap) ;

    Map out ;
    out.allocate(new_domain) ;
    // fill in the map contents
    for(size_t i=0;i<recv.size();++i) {
      int b = recv_displs[i] ;
      int e = b + recv_counts[i] ;
      for(;b<e;b+=2) {
        int en = remap[recv_buffer[b]] ; // only domain is remapped
        int im = recv_buffer[b+1] ;
        out[en] = im ;
      }
    }

    return out.Rep() ;
  }
  
  Loci::storeRepP
  get_remote_Map(const Map& in,
                 const ParticleSpace::Communicator& comm,
                 Loci::fact_db* factsP) {
    // extract the mapping info from the comm structure
    dMap remap(comm.unpack) ;

    return get_remote_Map(in, comm.send, comm.recv, remap, factsP) ;
  }

  // To be revised later
  Loci::storeRepP
  get_remote_Map(const Map& in,
                 const entitySet& domain,
                 const dMap& remap, Loci::fact_db* factsP) {
    // Loci::fact_db::distribute_infoP df = factsP->get_distribute_info() ;

    // ParticleSpace::Communicator comm ;
    // comm.generate_p2p_comm(domain, factsP->get_init_ptn(),
    //                        remap.Rep(), (df->l2g).Rep()) ;

    // return get_remote_Map(in, comm.send, comm.recv, remap, factsP) ;
    return 0 ;
  }

  Loci::storeRepP
  get_remote_multiMap(const Loci::multiMap& in,
                      const std::vector<P2pCommInfo>& send,
                      const std::vector<P2pCommInfo>& recv,
                      const dMap& remap, Loci::fact_db* factsP) {
    Loci::fact_db::distribute_infoP df = factsP->get_distribute_info() ;  
    // first we will need to communicate the size of the send buffer
    int total_send_size = 0 ;
    vector<int> send_counts(send.size(),0) ;
    for(size_t i=0;i<send.size();++i) {
      const entitySet& domain = send[i].entities_l ;
      for(entitySet::const_iterator ei=domain.begin();
          ei!=domain.end();++ei) {
        send_counts[i] += in[*ei].size() ;
        send_counts[i] += 2 ;   // for *ei and *ei size
      }
      total_send_size += send_counts[i] ;
    }

    vector<int> recv_counts(recv.size(),0) ;
    vector<MPI_Request> requests(recv.size()) ;

    // recv the size
    size_t req = 0 ;
    int local_offset = 0 ;
    for(size_t i=0;i<recv.size();++i) {
      if(recv[i].proc == Loci::MPI_rank) {
        local_offset = i ;
      } else {
        MPI_Irecv(&recv_counts[i], 1, MPI_INT, recv[i].proc,
                  recv[i].proc, MPI_COMM_WORLD, &requests[req++]) ;
      }
    }
    // send the size
    for(size_t i=0;i<send.size();++i) {
      if(send[i].proc == Loci::MPI_rank) {
        recv_counts[local_offset] = send_counts[i] ;
      } else {
        MPI_Send(&send_counts[i], 1, MPI_INT,
                 send[i].proc, Loci::MPI_rank, MPI_COMM_WORLD) ;
      }
    }
    // wait all Irecv
    if(req > 0) {
      MPI_Waitall(req, &requests[0], MPI_STATUSES_IGNORE) ;
    }

    vector<int> send_displs(send.size(), 0) ;
    if(!send.empty()) {
      send_displs[0] = 0 ;
      for(size_t i=1;i<send.size();++i)
        send_displs[i] = send_displs[i-1] + send_counts[i-1] ;
    }

    int total_recv_size = 0 ;
    vector<int> recv_displs(recv.size(), 0) ;
    if(!recv.empty()) {
      recv_displs[0] = 0 ;
      total_recv_size += recv_counts[0] ;
      for(size_t i=1;i<recv.size();++i) {
        recv_displs[i] = recv_displs[i-1] + recv_counts[i-1] ;
        total_recv_size += recv_counts[i] ;
      }
    }

    // pack the buffer
    vector<int> send_buffer(total_send_size) ;
    int idx = 0 ;
    for(size_t i=0;i<send.size();++i) {
      const entitySet& domain = send[i].entities_l ;
      for(entitySet::const_iterator ei=domain.begin();
          ei!=domain.end();++ei) {
        // first store the domain entity (in global numbering)
        send_buffer[idx++] = df->l2g[*ei] ;
        // then put the size
        int sz = in[*ei].size() ;
        send_buffer[idx++] = sz ;
        // elements also in global numbering
        for(int k=0;k<sz;++k)
          send_buffer[idx++] = df->l2g[in[*ei][k]] ;
      }
    }

    // allocate recv buffer
    vector<int> recv_buffer(total_recv_size) ;
    req = 0 ;
    // recv
    for(size_t i=0;i<recv.size();++i) {
      if(recv[i].proc == Loci::MPI_rank) {
        local_offset = recv_displs[i] ;
      } else {
        MPI_Irecv(&recv_buffer[recv_displs[i]],
                  recv_counts[i], MPI_INT, recv[i].proc,
                  recv[i].proc, MPI_COMM_WORLD, &requests[req++]) ;
      }
    }
    // send the buffer
    for(size_t i=0;i<send.size();++i) {
      if(send[i].proc == Loci::MPI_rank) {
        // do a memory copy instead of send
        for(int k=send_displs[i];k<send_displs[i]+send_counts[i];
            ++k,++local_offset)
          recv_buffer[local_offset] = send_buffer[k] ;
      } else {
        MPI_Send(&send_buffer[send_displs[i]],
                 send_counts[i], MPI_INT, send[i].proc,
                 Loci::MPI_rank, MPI_COMM_WORLD) ;
      }
    }
    // wait all Irecv
    if(req > 0) {
      MPI_Waitall(req, &requests[0], MPI_STATUSES_IGNORE) ;
    }

    // release the useless buffer
    vector<int>().swap(send_buffer) ;
    vector<int>().swap(send_counts) ;
    vector<int>().swap(send_displs) ;

    // extract the buffer
    // get the new domain first
    entitySet new_domain ;
    for(size_t i=0;i<recv.size();++i)
      new_domain += recv[i].entities_g ;
    // remap to new numbering
    new_domain = remap_entitySet(new_domain, remap) ;

    // create allocate domain
    store<int> alloc ;
    alloc.allocate(new_domain) ;
    // figure out the allocate size
    for(size_t i=0;i<recv.size();++i) {
      int b = recv_displs[i] ;
      int e = b + recv_counts[i] ;
      for(;b<e;) {
        int en = remap[recv_buffer[b]] ;
        int size = recv_buffer[b+1] ;
        alloc[en] = size ;
        b = b+2+size ;
      }
    }
    
    multiMap out ;
    out.allocate(alloc) ;
    // fill in the map contents
    for(size_t i=0;i<recv.size();++i) {
      int b = recv_displs[i] ;
      int e = b + recv_counts[i] ;
      for(;b<e;) {
        int en = remap[recv_buffer[b]] ;
        int size = recv_buffer[b+1] ;
        b+=2 ;
        for(int k=0;k<size;++k)
          out[en][k] = recv_buffer[b+k] ;
        b+=size ;
      }
    }

    return out.Rep() ;
    
  }

  // To be revised later
  Loci::storeRepP
  get_remote_multiMap(const Loci::multiMap& in,
                      const entitySet& domain,
                      const dMap& remap, Loci::fact_db* factsP) {
    // Loci::fact_db::distribute_infoP df = factsP->get_distribute_info() ;

    // ParticleSpace::Communicator comm ;
    // comm.generate_p2p_comm(domain, factsP->get_init_ptn(),
    //                        remap.Rep(), (df->l2g).Rep()) ;

    // return get_remote_multiMap(in, comm.send, comm.recv, remap, factsP) ;
    return 0 ;
  }
#endif
  
  void
  reduce_remote_stores(std::vector<Loci::storeRepP>& in_rep,
                       std::vector<store_traverserP>& unit_op,
                       std::vector<loci_joinerP>& join_op,
                       const std::vector<P2pCommInfo>& send,
                       const std::vector<P2pCommInfo>& recv,
                       const Map& pack_remap,
                       const dMap& unpack_remap,
                       std::vector<Loci::storeRepP>& out_rep) {
    // first we will need to communicate the size of the send buffer
    // to the receiving process so that they can allocate buffer.
    // we also need to communicate the pack entitySet in sequence
    // to the receiving processes so that they can properly unpack
    // the buffer.
    // normally, we need to send 3 messages, 1) the size of the
    // total packed buffer to send to a particular process, 2)
    // the size of the sequence to send, 3) the sequence itself.
    // in order to save message start-up time, we combine 1) and
    // 2) messages together into one message since they are both
    // integer type.
    vector<int> pack_size(send.size(),0) ;
    vector<int> seq_size(send.size(),0) ;
    vector<bool> pack_interval(send.size(),false) ;
    vector<sequence> pack_seq(send.size()) ;
    // compute pack size first
    for(size_t i=0;i<send.size();++i) {
      for(size_t k=0;k<in_rep.size();++k) {
        pack_size[i] += in_rep[k]->pack_size(send[i].entities_l) ;
      }
    }
    // compute the packing sequence in global numbering
    // and also the way to send the sequence (in intervals
    // or direct elements), and the sequence send size.
    for(size_t i=0;i<send.size();++i) {
      pack_seq[i] = remap_sequence(sequence(send[i].entities_l),
                                   pack_remap) ;
      int interval_size = pack_seq[i].num_intervals() ;
      int elem_size = pack_seq[i].size() ;
      if(2*interval_size < elem_size) {
        pack_interval[i] = true ;
        seq_size[i] = 2*interval_size + 1 ;
      } else {
        pack_interval[i] = false ;
        seq_size[i] = elem_size + 1 ;
      }
    }
    // now send the total pack size and seq size first
    vector<int> recv_msg_size(recv.size()*2,0) ;

    vector<MPI_Request> requests(recv.size()) ;
    // first post recv requests
    int req = 0 ;
    // this is used to optimize in the case of sending/receiving
    // messages to itself, we instead would just do a copy
    int self_msg_buffer_idx = -1 ;
    for(size_t i=0;i<recv.size();++i) {
      if(recv[i].proc == Loci::MPI_rank) {
        self_msg_buffer_idx = i ;
      } else {
        MPI_Irecv(&recv_msg_size[i*2], 2, MPI_INT, recv[i].proc,
                  recv[i].proc, MPI_COMM_WORLD, &requests[req++]) ;
      }
    }
    // then post send requests
    for(size_t i=0;i<send.size();++i) {
      if(send[i].proc == Loci::MPI_rank) {
        // just do a copy
        recv_msg_size[2*self_msg_buffer_idx] = pack_size[i] ;
        // this is another optimization, we do not need to
        // communicate the packing sequence for myself
        recv_msg_size[2*self_msg_buffer_idx+1] = 0 ;
      } else {
        int tmp[2] ;
        // first one is the total pack size
        tmp[0] = pack_size[i] ;
        // second one is the pack sequence size
        tmp[1] = seq_size[i] ;
        MPI_Send(tmp, 2, MPI_INT,
                 send[i].proc, Loci::MPI_rank, MPI_COMM_WORLD) ;
      }
    }
    // wait all Irecv to finish
    if(req > 0) {
      MPI_Waitall(req, &requests[0], MPI_STATUSES_IGNORE) ;
    }
    // then we actually need to communicate the packing sequence
    // to all the receiving processes

    // allocate recv buffer first
    int total_recv_size = 0 ;
    for(size_t i=0;i<recv.size();++i)
      total_recv_size += recv_msg_size[2*i+1] ;
    int* unpack_seq_recv_buffer = new int[total_recv_size] ;
    int** unpack_seq_recv_buffer_ptr = new int*[recv.size()] ;

    if(!recv.empty()) {
      unpack_seq_recv_buffer_ptr[0] = unpack_seq_recv_buffer ;
      for(size_t i=1;i<recv.size();++i)
        unpack_seq_recv_buffer_ptr[i] = recv_msg_size[2*(i-1)+1] +
          unpack_seq_recv_buffer_ptr[i-1] ;
    }
    
    // post recv requests
    req = 0 ;
    for(size_t i=0;i<recv.size();++i) {
      if(recv[i].proc != Loci::MPI_rank) {
        MPI_Irecv(unpack_seq_recv_buffer_ptr[i],
                  recv_msg_size[2*i+1],
                  MPI_INT, recv[i].proc,
                  recv[i].proc, MPI_COMM_WORLD, &requests[req++]) ;
      }
    }
    // do the send
    for(size_t i=0;i<send.size();++i) {
      if(send[i].proc != Loci::MPI_rank) {
        // allocate a send buffer
        vector<int> buf(seq_size[i]) ;
        // pack it
        if(pack_interval[i]) {
          // pack intervals
          buf[0] = 1 ;
          int count = 1 ;
          const sequence& seq = pack_seq[i] ;
          for(size_t k=0;k<seq.num_intervals();++k) {
            buf[count++] = seq[k].first ;
            buf[count++] = seq[k].second ;
          }
        } else {
          // pack elements
          buf[0] = 0 ;
          int count = 1 ;
          const sequence& seq = pack_seq[i] ;
          for(sequence::const_iterator si=seq.begin();
              si!=seq.end();++si,++count)
            buf[count] = *si ;
        }
        // send the buffer
        MPI_Send(&buf[0], seq_size[i], MPI_INT,
                 send[i].proc, Loci::MPI_rank, MPI_COMM_WORLD) ;
      } else {
        self_msg_buffer_idx = i ;
      }
    }
    // wait all Irecv to finish
    if(req > 0) {
      MPI_Waitall(req, &requests[0], MPI_STATUSES_IGNORE) ;
    }
    // then unpack the sequence buffer
    vector<sequence> unpack_seq(recv.size()) ;
    for(size_t i=0;i<recv.size();++i) {
      if(recv[i].proc == Loci::MPI_rank) {
        // just copy
        unpack_seq[i] = pack_seq[self_msg_buffer_idx] ;
      } else {
        // extract the first int to see if the packed
        // stuff is interval or element
        sequence& seq = unpack_seq[i] ;
        int* p = unpack_seq_recv_buffer_ptr[i] ;
        int size = recv_msg_size[2*i+1] - 1 ;
        if(*p == 1) {
          // extract intervals
          ++p ;
          for(int k=0;k<size;k+=2) {
            int b = *p ; ++p ;
            int e = *p ; ++p ;
            seq += interval(b,e) ;
          }
        } else {
          // extract elements
          ++p ;
          for(int k=0;k<size;++k,++p)
            seq += *p ;
        }
      }
    }
    // release all un-necessary buffers
    vector<int>().swap(seq_size) ;
    vector<bool>().swap(pack_interval) ;
    vector<sequence>().swap(pack_seq) ;
    delete[] unpack_seq_recv_buffer_ptr ;
    delete[] unpack_seq_recv_buffer ;

    // remap the unpack sequence to the new numbering
    for(size_t i=0;i<recv.size();++i) {
      unpack_seq[i] = remap_sequence(unpack_seq[i], unpack_remap) ;
    }
    
    // now we have enough information to send/recv the real
    // contents in the storeRepP

    // allocate a recv buffer first.
    total_recv_size = 0 ;
    for(size_t i=0;i<recv.size();++i)
      total_recv_size += recv_msg_size[2*i] ; // the unpack size for i
    unsigned char* unpack_buffer = new unsigned char[total_recv_size] ;
    unsigned char** unpack_buffer_ptr = new unsigned char*[recv.size()] ;

    if(!recv.empty()) {
      unpack_buffer_ptr[0] = unpack_buffer ;
      for(size_t i=1;i<recv.size();++i)
        unpack_buffer_ptr[i] = recv_msg_size[2*(i-1)] +
          unpack_buffer_ptr[i-1] ;
    }
    // post recv requests
    req = 0 ;
    for(size_t i=0;i<recv.size();++i) {
      if(recv[i].proc == Loci::MPI_rank) {
        self_msg_buffer_idx = i ;
      } else {
        MPI_Irecv(unpack_buffer_ptr[i], recv_msg_size[2*i],
                  MPI_PACKED, recv[i].proc, recv[i].proc,
                  MPI_COMM_WORLD, &requests[req++]) ;
      }
    }
    // actually pack and send
    for(size_t i=0;i<send.size();++i) {
      if(send[i].proc == Loci::MPI_rank) {
        unsigned char* pack_buffer =
          unpack_buffer_ptr[self_msg_buffer_idx] ;
        int position = 0 ;
        for(size_t k=0;k<in_rep.size();++k) {
          in_rep[k]->pack(pack_buffer, position,
                          pack_size[i], send[i].entities_l) ;
        }
        
      } else {
        // first allocate the pack buffer
        unsigned char* pack_buffer = new unsigned char[pack_size[i]] ;
        // then do the pack
        int position = 0 ;
        for(size_t k=0;k<in_rep.size();++k) {
          // pack the contents
          in_rep[k]->pack(pack_buffer, position,
                          pack_size[i], send[i].entities_l) ;
        }
        // done all the packing, send it
        MPI_Send(pack_buffer, pack_size[i], MPI_PACKED,
                 send[i].proc, Loci::MPI_rank, MPI_COMM_WORLD) ;
        // destroy the send buffer
        delete[] pack_buffer ;
      }
    }
    // wait all Irecv to finish
    if(req > 0) {
      MPI_Waitall(req, &requests[0], MPI_STATUSES_IGNORE) ;
    }

    // then unpack the receiving buffer

    // first compute the new domain for all the out storeRepP
    entitySet new_domain ;
    for(size_t i=0;i<recv.size();++i) {
      new_domain += recv[i].entities_g ;
    }
    // remap it to the new numbering
    new_domain = remap_entitySet(new_domain, unpack_remap) ;
    // the new_domain in the sequence form
    sequence new_domain_seq = sequence(new_domain) ;

    // allocate all storeRepP in out_rep
    if(out_rep.size() != in_rep.size())
      out_rep.resize(in_rep.size()) ;

    // also create a tmp storeRepP vector for reduction purpose
    vector<Loci::storeRepP> tmp_rep(out_rep.size()) ;

    for(size_t i=0;i<out_rep.size();++i) {
      out_rep[i] = in_rep[i]->new_store(new_domain) ;
      // we will also need to set the unit value
      unit_op[i]->set_args(out_rep[i]) ;
      unit_op[i]->traverse(new_domain_seq) ;
      tmp_rep[i] = in_rep[i]->new_store(new_domain) ;
    }
    
    // and finally we will unpack the buffer and perform reduction
    for(size_t i=0;i<recv.size();++i) {
      int position = 0 ;
      int unpack_buffer_size = recv_msg_size[2*i] ;
      for(size_t k=0;k<out_rep.size();++k) {
        // unpack the contents to tmp_rep first
        tmp_rep[k]->unpack(unpack_buffer_ptr[i], position,
                           unpack_buffer_size, unpack_seq[i]) ;
        // then join the contents to out_rep
        join_op[k]->SetArgs(out_rep[k], /*target rep*/
                            tmp_rep[k]  /*source rep*/) ;
        join_op[k]->Join(unpack_seq[i]) ;
      }
    }
    // release recv buffer and finish up
    delete[] unpack_buffer_ptr ;
    delete[] unpack_buffer ;
  }

  void
  reduce_remote_stores(std::vector<Loci::storeRepP>& in_rep,
                       std::vector<store_traverserP>& unit_op,
                       std::vector<loci_joinerP>& join_op,
                       std::vector<Loci::storeRepP>& out_rep,
                       const ParticleSpace::Communicator& comm) {
    Map pack_remap ; pack_remap.setRep(comm.pack.Rep()) ;
    dMap unpack_remap ; unpack_remap.setRep(comm.unpack.Rep()) ;

    reduce_remote_stores(in_rep, unit_op, join_op, comm.send,
                         comm.recv, pack_remap, unpack_remap, out_rep) ;
  }

  Loci::storeRepP
  reduce_remote_stores(Loci::storeRepP in_rep,
                       store_traverserP unit_op,
                       loci_joinerP join_op,
                       const std::vector<P2pCommInfo>& send,
                       const std::vector<P2pCommInfo>& recv,
                       const Map& pack_remap,
                       const dMap& unpack_remap) {
    std::vector<Loci::storeRepP> rep_v_in(1, in_rep), rep_v_out(1) ;
    std::vector<store_traverserP> unit_v(1, unit_op) ;
    std::vector<loci_joinerP> join_v(1, join_op) ;

    reduce_remote_stores(rep_v_in, unit_v, join_v,
                         send, recv, pack_remap, unpack_remap, rep_v_out) ;

    return rep_v_out[0] ;
  }

  Loci::storeRepP
  reduce_remote_stores(Loci::storeRepP in_rep,
                       store_traverserP unit_op,
                       loci_joinerP join_op,
                       const ParticleSpace::Communicator& comm) {
    Map pack_remap ; pack_remap.setRep(comm.pack.Rep()) ;
    dMap unpack_remap ; unpack_remap.setRep(comm.unpack.Rep()) ;

    return reduce_remote_stores(in_rep, unit_op, join_op, comm.send,
                                comm.recv, pack_remap, unpack_remap) ;
  }
  
} 

// ... the end ...
